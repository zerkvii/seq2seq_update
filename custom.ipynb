{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt16 (/home/gaojunting/.cache/huggingface/datasets/wmt16/cs-en/1.0.0/28ebdf8cf22106c2f1e58b2083d4b103608acd7bfdb6b14313ccd9e5bc8c313a)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016196727752685547,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce538c1ec1db40ccbbbfd1343c55cacb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('wmt16','cs-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 997240\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2656\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data['train'][:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data=data['validation']\n",
    "test_data=data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSet(Dataset):\n",
    "    def __init__(self,raw_data):\n",
    "        self.raw_data=raw_data['translation']\n",
    "    def __getitem__(self,idx):\n",
    "        return self.raw_data[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=CustomSet(train_data)\n",
    "val_ds=CustomSet(val_data)\n",
    "test_ds=CustomSet(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<EOS>\", \"<PAD>\", \"<SOS>\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_tokenizer,cs_trainer=prepare_tokenizer_trainer('BPE')\n",
    "en_tokenizer,en_trainer=prepare_tokenizer_trainer('BPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_lang=[item['cs'] for item in train_ds] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lang=[item['en'] for item in train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cs_tokenizer.train_from_iterator(cs_lang,trainer=cs_trainer)\n",
    "en_tokenizer.train_from_iterator(en_lang,trainer=en_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Následný postup na základě usnesení Parlamentu: viz zápis'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['cs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14044, 1183, 194, 1204, 1177, 822, 26, 1343, 1427]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_tokenizer.encode(a['cs']).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_id=cs_tokenizer.token_to_id('<PAD>')\n",
    "trg_pad_id=en_tokenizer.token_to_id('<PAD>')\n",
    "src_eos_id=cs_tokenizer.token_to_id('<EOS>')\n",
    "src_sos_id=cs_tokenizer.token_to_id('<SOS>')\n",
    "trg_eos_id=en_tokenizer.token_to_id('<EOS>')\n",
    "trg_sos_id=en_tokenizer.token_to_id('<SOS>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pad_id,src_sos_id,src_eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_pad_id,trg_sos_id,trg_eos_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_tokenizer.enable_padding(length=50,pad_id=src_pad_id)\n",
    "cs_tokenizer.enable_truncation(max_length=50)\n",
    "en_tokenizer.enable_padding(length=50,pad_id=trg_pad_id)\n",
    "en_tokenizer.enable_truncation(max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    cs_ls,en_ls=[],[]\n",
    "    for item in batch:\n",
    "        cs_sent='<SOS> '+item['cs']+' <EOS>'\n",
    "        en_sent='<SOS> '+item['en']+' <EOS>'\n",
    "        cs_ls.append(torch.LongTensor(cs_tokenizer.encode(cs_sent).ids))\n",
    "        en_ls.append(torch.LongTensor(en_tokenizer.encode(en_sent).ids))\n",
    "    return torch.vstack(cs_ls),torch.vstack(en_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=DataLoader(dataset=train_ds,batch_size=64,shuffle=True,collate_fn=collate_fn)\n",
    "valid_loader=DataLoader(dataset=val_ds,batch_size=64,shuffle=True,collate_fn=collate_fn)\n",
    "test_loader=DataLoader(dataset=test_ds,batch_size=64,shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    3, 19228,  1306,   195,  1230,  1620,  7881,   194,  1790,    12,\n",
       "          940,   195,   837,    12,   740,   362,  2765,  2058,    12,   508,\n",
       "         7734,    14,     1,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "            2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self,hid_dim,n_heads,dropout,device):\n",
    "        super().__init__()\n",
    "        assert hid_dim%n_heads==0\n",
    "        self.hid_dim=hid_dim\n",
    "        self.n_heads=n_heads\n",
    "        self.head_dim=hid_dim//n_heads\n",
    "        self.q=nn.Linear(hid_dim,hid_dim)\n",
    "        self.k=nn.Linear(hid_dim,hid_dim)\n",
    "        self.v=nn.Linear(hid_dim,hid_dim)\n",
    "        self.o=nn.Linear(hid_dim,hid_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.scale=torch.sqrt(torch.LongTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self,query,key,value,mask=None):\n",
    "        batch_size=query.shape[0]\n",
    "        Q=self.q(query)\n",
    "        K=self.k(key)\n",
    "        V=self.v(value)\n",
    "        #K V is the same size while Q may not\n",
    "\n",
    "        Q=Q.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        K=K.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        V=V.view(batch_size,-1,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "\n",
    "        energy=(Q@K.permute(0,1,3,2))/self.scale\n",
    "\n",
    "        if mask is not None:\n",
    "            energy=energy.masked_fill(mask==0,-1e10)\n",
    "        attention = torch.softmax(energy,dim=-1)\n",
    "\n",
    "        x = self.dropout(attention) @ V\n",
    "        x = x.permute(0,2,1,3).contiguous()\n",
    "        x = x.view(batch_size,-1,self.hid_dim)\n",
    "        x = self.o(x)\n",
    "        return x,attention\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseForwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        \n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,hid_dim,n_heads,pf_dim,dropout,device):\n",
    "        super().__init__()\n",
    "        self.self_att_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm=nn.LayerNorm(hid_dim)\n",
    "        self.self_attn=MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
    "        self.positionwise_feedforward=PositionwiseForwardLayer(hid_dim,pf_dim,dropout)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,src,src_mask):\n",
    "        _src,_=self.self_attn(src,src,src,src_mask)\n",
    "        src = self.self_att_layer_norm(src+self.dropout(_src))\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        src=self.ff_layer_norm(src+self.dropout(_src))\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length=50):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.embedding = nn.Embedding(input_dim,hid_dim)\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim)\n",
    "        self.layers=nn.ModuleList([EncoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale=torch.FloatTensor([hid_dim]).to(device)\n",
    "    def forward(self,src,src_mask):\n",
    "        #batch first\n",
    "        batch_size=src.shape[0]\n",
    "        src_len=src.shape[1]\n",
    "        pos=torch.arange(0, src_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        \n",
    "        src = self.dropout((self.embedding(src)*self.scale)+self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src=layer(src,src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_tokenizer(files, alg='WLV'):\n",
    "#     \"\"\"\n",
    "#     Takes the files and trains the tokenizer.\n",
    "#     \"\"\"\n",
    "#     tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "#     tokenizer.train(files, trainer) # training the tokenzier\n",
    "#     tokenizer.save(\"./tokenizer-trained.json\")\n",
    "#     tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
    "#     return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,hid_dim,n_heads,pf_dim,dropout,device) -> None:\n",
    "        super().__init__()\n",
    "        self.multi_att=MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
    "        self.multi_att_norm=nn.LayerNorm(hid_dim)\n",
    "        self.feed_forward_layer=PositionwiseForwardLayer(hid_dim,pf_dim,dropout)\n",
    "        self.feed_norm=nn.LayerNorm(hid_dim)\n",
    "\n",
    "        self.encoder_attn=MultiHeadAttentionLayer(hid_dim,n_heads,dropout,device)\n",
    "        self.encoder_norm=nn.LayerNorm(hid_dim)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,trg,en_src,trg_mask,enc_mask):\n",
    "\n",
    "        _trg,_=self.multi_att(trg,trg,trg,trg_mask)\n",
    "        trg=self.multi_att_norm(trg+self.dropout(_trg))\n",
    "\n",
    "        _trg,att=self.encoder_attn(trg,en_src,en_src,enc_mask)\n",
    "        trg = self.encoder_norm(trg+self.dropout(_trg))\n",
    "\n",
    "        _trg=self.feed_forward_layer(trg)\n",
    "        trg=self.feed_norm(trg+self.dropout(_trg))\n",
    "\n",
    "        return trg,att\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim,hid_dim,n_layers,n_heads,pf_dim,dropout,device,max_length=50):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "\n",
    "        self.embedding=nn.Embedding(output_dim,hid_dim)\n",
    "        self.pos_embedding=nn.Embedding(max_length,hid_dim)\n",
    "\n",
    "        self.layers=nn.ModuleList([DecoderLayer(hid_dim,n_heads,pf_dim,dropout,device) for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out=nn.Linear(hid_dim,output_dim)\n",
    "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,trg,enc_src,trg_mask,enc_mask):\n",
    "        batch_size=trg.shape[0]\n",
    "        trg_len=trg.shape[1]\n",
    "\n",
    "        pos=torch.arange(0,trg_len).unsqueeze(0).repeat(batch_size,1).to(self.device)\n",
    "        trg=self.dropout(self.embedding(trg)*self.scale +self.pos_embedding(pos))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg,attention=layer(trg,enc_src,trg_mask,enc_mask)\n",
    "        output=self.fc_out(trg)\n",
    "        return output,attention\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,src_pad_idx,trg_pad_idx,device):\n",
    "        super().__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        self.src_pad_idx=src_pad_idx\n",
    "        self.trg_pad_idx=trg_pad_idx\n",
    "        self.device=device\n",
    "    def make_src_mask(self,src):\n",
    "        src_mask=(src!=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    def make_trg_mask(self,trg):\n",
    "        trg_pad_mask=(trg!=self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        trg_len=trg.shape[1]\n",
    "        trg_sub_mask=torch.tril(torch.ones(trg_len,trg_len)).bool().to(self.device)\n",
    "        #batch_size 1 seq_len seq_len\n",
    "        trg_mask=trg_pad_mask&trg_sub_mask\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self,src,trg):\n",
    "        src_mask=self.make_src_mask(src)\n",
    "        trg_mask=self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src=self.encoder(src,src_mask)\n",
    "\n",
    "        output,attention=self.decoder(trg,enc_src,trg_mask,src_mask)\n",
    "\n",
    "        return output,attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d=cs_tokenizer.get_vocab_size()\n",
    "output_d=en_tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = input_d\n",
    "OUTPUT_DIM = output_d\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=nn.Transformer(d_model=HID_DIM,nhead=ENC_HEADS,dim_feedforward=ENC_PF_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Seq2SeqTransformer(num_encoder_layers=ENC_LAYERS,\n",
    "                         num_decoder_layers=DEC_LAYERS,\n",
    "                         src_vocab_size=INPUT_DIM,\n",
    "                         emb_size=HID_DIM,\n",
    "                         nhead=ENC_HEADS,\n",
    "                         tgt_vocab_size=OUTPUT_DIM,\n",
    "                         dropout=ENC_DROPOUT\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (torch.tril(torch.ones((12, 12))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == src_pad_id).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == trg_pad_id).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder=Encoder(INPUT_DIM,HID_DIM,ENC_LAYERS,ENC_HEADS,ENC_PF_DIM,ENC_DROPOUT,device)\n",
    "# decoder=Decoder(OUTPUT_DIM,HID_DIM,DEC_LAYERS,DEC_HEADS,DEC_PF_DIM,DEC_DROPOUT,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Seq2Seq(encoder,decoder,src_pad_id,trg_pad_id,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 27,024,688 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (generator): Linear(in_features=256, out_features=30000, bias=True)\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (tgt_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(30000, 256)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = trg_pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 49])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[1][:,:-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch[0].to(device).T\n",
    "        trg = batch[1].to(device).T\n",
    "        train_trg=trg[:-1,:]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, train_trg)\n",
    "        optimizer.zero_grad()\n",
    "        print(src.size(1))\n",
    "        print(trg.size(1))\n",
    "        \n",
    "        output= model(src,train_trg,src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        print(output.shape)\n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[1:,:].contiguous().view(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "\n",
    "            src = batch[0].to(device).T\n",
    "            trg = batch[1].to(device).T\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            #batch first\n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n",
      "64\n",
      "64\n",
      "torch.Size([49, 64, 30000])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [250]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[1;32m      8\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion)\n\u001b[1;32m     13\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [249]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#output = [batch size * trg len - 1, output dim]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#trg = [batch size * trg len - 1]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2SeqTransformer:\n\tMissing key(s) in state_dict: \"transformer.encoder.layers.0.self_attn.in_proj_weight\", \"transformer.encoder.layers.0.self_attn.in_proj_bias\", \"transformer.encoder.layers.0.self_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.out_proj.bias\", \"transformer.encoder.layers.0.linear1.weight\", \"transformer.encoder.layers.0.linear1.bias\", \"transformer.encoder.layers.0.linear2.weight\", \"transformer.encoder.layers.0.linear2.bias\", \"transformer.encoder.layers.0.norm1.weight\", \"transformer.encoder.layers.0.norm1.bias\", \"transformer.encoder.layers.0.norm2.weight\", \"transformer.encoder.layers.0.norm2.bias\", \"transformer.encoder.layers.1.self_attn.in_proj_weight\", \"transformer.encoder.layers.1.self_attn.in_proj_bias\", \"transformer.encoder.layers.1.self_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.out_proj.bias\", \"transformer.encoder.layers.1.linear1.weight\", \"transformer.encoder.layers.1.linear1.bias\", \"transformer.encoder.layers.1.linear2.weight\", \"transformer.encoder.layers.1.linear2.bias\", \"transformer.encoder.layers.1.norm1.weight\", \"transformer.encoder.layers.1.norm1.bias\", \"transformer.encoder.layers.1.norm2.weight\", \"transformer.encoder.layers.1.norm2.bias\", \"transformer.encoder.layers.2.self_attn.in_proj_weight\", \"transformer.encoder.layers.2.self_attn.in_proj_bias\", \"transformer.encoder.layers.2.self_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.out_proj.bias\", \"transformer.encoder.layers.2.linear1.weight\", \"transformer.encoder.layers.2.linear1.bias\", \"transformer.encoder.layers.2.linear2.weight\", \"transformer.encoder.layers.2.linear2.bias\", \"transformer.encoder.layers.2.norm1.weight\", \"transformer.encoder.layers.2.norm1.bias\", \"transformer.encoder.layers.2.norm2.weight\", \"transformer.encoder.layers.2.norm2.bias\", \"transformer.encoder.norm.weight\", \"transformer.encoder.norm.bias\", \"transformer.decoder.layers.0.self_attn.in_proj_weight\", \"transformer.decoder.layers.0.self_attn.in_proj_bias\", \"transformer.decoder.layers.0.self_attn.out_proj.weight\", \"transformer.decoder.layers.0.self_attn.out_proj.bias\", \"transformer.decoder.layers.0.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.0.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.0.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.0.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.0.linear1.weight\", \"transformer.decoder.layers.0.linear1.bias\", \"transformer.decoder.layers.0.linear2.weight\", \"transformer.decoder.layers.0.linear2.bias\", \"transformer.decoder.layers.0.norm1.weight\", \"transformer.decoder.layers.0.norm1.bias\", \"transformer.decoder.layers.0.norm2.weight\", \"transformer.decoder.layers.0.norm2.bias\", \"transformer.decoder.layers.0.norm3.weight\", \"transformer.decoder.layers.0.norm3.bias\", \"transformer.decoder.layers.1.self_attn.in_proj_weight\", \"transformer.decoder.layers.1.self_attn.in_proj_bias\", \"transformer.decoder.layers.1.self_attn.out_proj.weight\", \"transformer.decoder.layers.1.self_attn.out_proj.bias\", \"transformer.decoder.layers.1.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.1.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.1.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.1.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.1.linear1.weight\", \"transformer.decoder.layers.1.linear1.bias\", \"transformer.decoder.layers.1.linear2.weight\", \"transformer.decoder.layers.1.linear2.bias\", \"transformer.decoder.layers.1.norm1.weight\", \"transformer.decoder.layers.1.norm1.bias\", \"transformer.decoder.layers.1.norm2.weight\", \"transformer.decoder.layers.1.norm2.bias\", \"transformer.decoder.layers.1.norm3.weight\", \"transformer.decoder.layers.1.norm3.bias\", \"transformer.decoder.layers.2.self_attn.in_proj_weight\", \"transformer.decoder.layers.2.self_attn.in_proj_bias\", \"transformer.decoder.layers.2.self_attn.out_proj.weight\", \"transformer.decoder.layers.2.self_attn.out_proj.bias\", \"transformer.decoder.layers.2.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.2.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.2.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.2.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.2.linear1.weight\", \"transformer.decoder.layers.2.linear1.bias\", \"transformer.decoder.layers.2.linear2.weight\", \"transformer.decoder.layers.2.linear2.bias\", \"transformer.decoder.layers.2.norm1.weight\", \"transformer.decoder.layers.2.norm1.bias\", \"transformer.decoder.layers.2.norm2.weight\", \"transformer.decoder.layers.2.norm2.bias\", \"transformer.decoder.layers.2.norm3.weight\", \"transformer.decoder.layers.2.norm3.bias\", \"transformer.decoder.norm.weight\", \"transformer.decoder.norm.bias\", \"generator.weight\", \"generator.bias\", \"src_tok_emb.embedding.weight\", \"tgt_tok_emb.embedding.weight\", \"positional_encoding.pos_embedding\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\", \"encoder.pos_embedding.weight\", \"encoder.layers.0.self_att_layer_norm.weight\", \"encoder.layers.0.self_att_layer_norm.bias\", \"encoder.layers.0.ff_layer_norm.weight\", \"encoder.layers.0.ff_layer_norm.bias\", \"encoder.layers.0.self_attn.q.weight\", \"encoder.layers.0.self_attn.q.bias\", \"encoder.layers.0.self_attn.k.weight\", \"encoder.layers.0.self_attn.k.bias\", \"encoder.layers.0.self_attn.v.weight\", \"encoder.layers.0.self_attn.v.bias\", \"encoder.layers.0.self_attn.o.weight\", \"encoder.layers.0.self_attn.o.bias\", \"encoder.layers.0.positionwise_feedforward.fc_1.weight\", \"encoder.layers.0.positionwise_feedforward.fc_1.bias\", \"encoder.layers.0.positionwise_feedforward.fc_2.weight\", \"encoder.layers.0.positionwise_feedforward.fc_2.bias\", \"encoder.layers.1.self_att_layer_norm.weight\", \"encoder.layers.1.self_att_layer_norm.bias\", \"encoder.layers.1.ff_layer_norm.weight\", \"encoder.layers.1.ff_layer_norm.bias\", \"encoder.layers.1.self_attn.q.weight\", \"encoder.layers.1.self_attn.q.bias\", \"encoder.layers.1.self_attn.k.weight\", \"encoder.layers.1.self_attn.k.bias\", \"encoder.layers.1.self_attn.v.weight\", \"encoder.layers.1.self_attn.v.bias\", \"encoder.layers.1.self_attn.o.weight\", \"encoder.layers.1.self_attn.o.bias\", \"encoder.layers.1.positionwise_feedforward.fc_1.weight\", \"encoder.layers.1.positionwise_feedforward.fc_1.bias\", \"encoder.layers.1.positionwise_feedforward.fc_2.weight\", \"encoder.layers.1.positionwise_feedforward.fc_2.bias\", \"encoder.layers.2.self_att_layer_norm.weight\", \"encoder.layers.2.self_att_layer_norm.bias\", \"encoder.layers.2.ff_layer_norm.weight\", \"encoder.layers.2.ff_layer_norm.bias\", \"encoder.layers.2.self_attn.q.weight\", \"encoder.layers.2.self_attn.q.bias\", \"encoder.layers.2.self_attn.k.weight\", \"encoder.layers.2.self_attn.k.bias\", \"encoder.layers.2.self_attn.v.weight\", \"encoder.layers.2.self_attn.v.bias\", \"encoder.layers.2.self_attn.o.weight\", \"encoder.layers.2.self_attn.o.bias\", \"encoder.layers.2.positionwise_feedforward.fc_1.weight\", \"encoder.layers.2.positionwise_feedforward.fc_1.bias\", \"encoder.layers.2.positionwise_feedforward.fc_2.weight\", \"encoder.layers.2.positionwise_feedforward.fc_2.bias\", \"decoder.embedding.weight\", \"decoder.pos_embedding.weight\", \"decoder.layers.0.multi_att.q.weight\", \"decoder.layers.0.multi_att.q.bias\", \"decoder.layers.0.multi_att.k.weight\", \"decoder.layers.0.multi_att.k.bias\", \"decoder.layers.0.multi_att.v.weight\", \"decoder.layers.0.multi_att.v.bias\", \"decoder.layers.0.multi_att.o.weight\", \"decoder.layers.0.multi_att.o.bias\", \"decoder.layers.0.multi_att_norm.weight\", \"decoder.layers.0.multi_att_norm.bias\", \"decoder.layers.0.feed_forward_layer.fc_1.weight\", \"decoder.layers.0.feed_forward_layer.fc_1.bias\", \"decoder.layers.0.feed_forward_layer.fc_2.weight\", \"decoder.layers.0.feed_forward_layer.fc_2.bias\", \"decoder.layers.0.feed_norm.weight\", \"decoder.layers.0.feed_norm.bias\", \"decoder.layers.0.encoder_attn.q.weight\", \"decoder.layers.0.encoder_attn.q.bias\", \"decoder.layers.0.encoder_attn.k.weight\", \"decoder.layers.0.encoder_attn.k.bias\", \"decoder.layers.0.encoder_attn.v.weight\", \"decoder.layers.0.encoder_attn.v.bias\", \"decoder.layers.0.encoder_attn.o.weight\", \"decoder.layers.0.encoder_attn.o.bias\", \"decoder.layers.0.encoder_norm.weight\", \"decoder.layers.0.encoder_norm.bias\", \"decoder.layers.1.multi_att.q.weight\", \"decoder.layers.1.multi_att.q.bias\", \"decoder.layers.1.multi_att.k.weight\", \"decoder.layers.1.multi_att.k.bias\", \"decoder.layers.1.multi_att.v.weight\", \"decoder.layers.1.multi_att.v.bias\", \"decoder.layers.1.multi_att.o.weight\", \"decoder.layers.1.multi_att.o.bias\", \"decoder.layers.1.multi_att_norm.weight\", \"decoder.layers.1.multi_att_norm.bias\", \"decoder.layers.1.feed_forward_layer.fc_1.weight\", \"decoder.layers.1.feed_forward_layer.fc_1.bias\", \"decoder.layers.1.feed_forward_layer.fc_2.weight\", \"decoder.layers.1.feed_forward_layer.fc_2.bias\", \"decoder.layers.1.feed_norm.weight\", \"decoder.layers.1.feed_norm.bias\", \"decoder.layers.1.encoder_attn.q.weight\", \"decoder.layers.1.encoder_attn.q.bias\", \"decoder.layers.1.encoder_attn.k.weight\", \"decoder.layers.1.encoder_attn.k.bias\", \"decoder.layers.1.encoder_attn.v.weight\", \"decoder.layers.1.encoder_attn.v.bias\", \"decoder.layers.1.encoder_attn.o.weight\", \"decoder.layers.1.encoder_attn.o.bias\", \"decoder.layers.1.encoder_norm.weight\", \"decoder.layers.1.encoder_norm.bias\", \"decoder.layers.2.multi_att.q.weight\", \"decoder.layers.2.multi_att.q.bias\", \"decoder.layers.2.multi_att.k.weight\", \"decoder.layers.2.multi_att.k.bias\", \"decoder.layers.2.multi_att.v.weight\", \"decoder.layers.2.multi_att.v.bias\", \"decoder.layers.2.multi_att.o.weight\", \"decoder.layers.2.multi_att.o.bias\", \"decoder.layers.2.multi_att_norm.weight\", \"decoder.layers.2.multi_att_norm.bias\", \"decoder.layers.2.feed_forward_layer.fc_1.weight\", \"decoder.layers.2.feed_forward_layer.fc_1.bias\", \"decoder.layers.2.feed_forward_layer.fc_2.weight\", \"decoder.layers.2.feed_forward_layer.fc_2.bias\", \"decoder.layers.2.feed_norm.weight\", \"decoder.layers.2.feed_norm.bias\", \"decoder.layers.2.encoder_attn.q.weight\", \"decoder.layers.2.encoder_attn.q.bias\", \"decoder.layers.2.encoder_attn.k.weight\", \"decoder.layers.2.encoder_attn.k.bias\", \"decoder.layers.2.encoder_attn.v.weight\", \"decoder.layers.2.encoder_attn.v.bias\", \"decoder.layers.2.encoder_attn.o.weight\", \"decoder.layers.2.encoder_attn.o.bias\", \"decoder.layers.2.encoder_norm.weight\", \"decoder.layers.2.encoder_norm.bias\", \"decoder.fc_out.weight\", \"decoder.fc_out.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [216]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtut6-model.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, criterion)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test PPL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mexp(test_loss)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m7.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2SeqTransformer:\n\tMissing key(s) in state_dict: \"transformer.encoder.layers.0.self_attn.in_proj_weight\", \"transformer.encoder.layers.0.self_attn.in_proj_bias\", \"transformer.encoder.layers.0.self_attn.out_proj.weight\", \"transformer.encoder.layers.0.self_attn.out_proj.bias\", \"transformer.encoder.layers.0.linear1.weight\", \"transformer.encoder.layers.0.linear1.bias\", \"transformer.encoder.layers.0.linear2.weight\", \"transformer.encoder.layers.0.linear2.bias\", \"transformer.encoder.layers.0.norm1.weight\", \"transformer.encoder.layers.0.norm1.bias\", \"transformer.encoder.layers.0.norm2.weight\", \"transformer.encoder.layers.0.norm2.bias\", \"transformer.encoder.layers.1.self_attn.in_proj_weight\", \"transformer.encoder.layers.1.self_attn.in_proj_bias\", \"transformer.encoder.layers.1.self_attn.out_proj.weight\", \"transformer.encoder.layers.1.self_attn.out_proj.bias\", \"transformer.encoder.layers.1.linear1.weight\", \"transformer.encoder.layers.1.linear1.bias\", \"transformer.encoder.layers.1.linear2.weight\", \"transformer.encoder.layers.1.linear2.bias\", \"transformer.encoder.layers.1.norm1.weight\", \"transformer.encoder.layers.1.norm1.bias\", \"transformer.encoder.layers.1.norm2.weight\", \"transformer.encoder.layers.1.norm2.bias\", \"transformer.encoder.layers.2.self_attn.in_proj_weight\", \"transformer.encoder.layers.2.self_attn.in_proj_bias\", \"transformer.encoder.layers.2.self_attn.out_proj.weight\", \"transformer.encoder.layers.2.self_attn.out_proj.bias\", \"transformer.encoder.layers.2.linear1.weight\", \"transformer.encoder.layers.2.linear1.bias\", \"transformer.encoder.layers.2.linear2.weight\", \"transformer.encoder.layers.2.linear2.bias\", \"transformer.encoder.layers.2.norm1.weight\", \"transformer.encoder.layers.2.norm1.bias\", \"transformer.encoder.layers.2.norm2.weight\", \"transformer.encoder.layers.2.norm2.bias\", \"transformer.encoder.norm.weight\", \"transformer.encoder.norm.bias\", \"transformer.decoder.layers.0.self_attn.in_proj_weight\", \"transformer.decoder.layers.0.self_attn.in_proj_bias\", \"transformer.decoder.layers.0.self_attn.out_proj.weight\", \"transformer.decoder.layers.0.self_attn.out_proj.bias\", \"transformer.decoder.layers.0.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.0.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.0.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.0.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.0.linear1.weight\", \"transformer.decoder.layers.0.linear1.bias\", \"transformer.decoder.layers.0.linear2.weight\", \"transformer.decoder.layers.0.linear2.bias\", \"transformer.decoder.layers.0.norm1.weight\", \"transformer.decoder.layers.0.norm1.bias\", \"transformer.decoder.layers.0.norm2.weight\", \"transformer.decoder.layers.0.norm2.bias\", \"transformer.decoder.layers.0.norm3.weight\", \"transformer.decoder.layers.0.norm3.bias\", \"transformer.decoder.layers.1.self_attn.in_proj_weight\", \"transformer.decoder.layers.1.self_attn.in_proj_bias\", \"transformer.decoder.layers.1.self_attn.out_proj.weight\", \"transformer.decoder.layers.1.self_attn.out_proj.bias\", \"transformer.decoder.layers.1.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.1.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.1.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.1.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.1.linear1.weight\", \"transformer.decoder.layers.1.linear1.bias\", \"transformer.decoder.layers.1.linear2.weight\", \"transformer.decoder.layers.1.linear2.bias\", \"transformer.decoder.layers.1.norm1.weight\", \"transformer.decoder.layers.1.norm1.bias\", \"transformer.decoder.layers.1.norm2.weight\", \"transformer.decoder.layers.1.norm2.bias\", \"transformer.decoder.layers.1.norm3.weight\", \"transformer.decoder.layers.1.norm3.bias\", \"transformer.decoder.layers.2.self_attn.in_proj_weight\", \"transformer.decoder.layers.2.self_attn.in_proj_bias\", \"transformer.decoder.layers.2.self_attn.out_proj.weight\", \"transformer.decoder.layers.2.self_attn.out_proj.bias\", \"transformer.decoder.layers.2.multihead_attn.in_proj_weight\", \"transformer.decoder.layers.2.multihead_attn.in_proj_bias\", \"transformer.decoder.layers.2.multihead_attn.out_proj.weight\", \"transformer.decoder.layers.2.multihead_attn.out_proj.bias\", \"transformer.decoder.layers.2.linear1.weight\", \"transformer.decoder.layers.2.linear1.bias\", \"transformer.decoder.layers.2.linear2.weight\", \"transformer.decoder.layers.2.linear2.bias\", \"transformer.decoder.layers.2.norm1.weight\", \"transformer.decoder.layers.2.norm1.bias\", \"transformer.decoder.layers.2.norm2.weight\", \"transformer.decoder.layers.2.norm2.bias\", \"transformer.decoder.layers.2.norm3.weight\", \"transformer.decoder.layers.2.norm3.bias\", \"transformer.decoder.norm.weight\", \"transformer.decoder.norm.bias\", \"generator.weight\", \"generator.bias\", \"src_tok_emb.embedding.weight\", \"tgt_tok_emb.embedding.weight\", \"positional_encoding.pos_embedding\". \n\tUnexpected key(s) in state_dict: \"encoder.embedding.weight\", \"encoder.pos_embedding.weight\", \"encoder.layers.0.self_att_layer_norm.weight\", \"encoder.layers.0.self_att_layer_norm.bias\", \"encoder.layers.0.ff_layer_norm.weight\", \"encoder.layers.0.ff_layer_norm.bias\", \"encoder.layers.0.self_attn.q.weight\", \"encoder.layers.0.self_attn.q.bias\", \"encoder.layers.0.self_attn.k.weight\", \"encoder.layers.0.self_attn.k.bias\", \"encoder.layers.0.self_attn.v.weight\", \"encoder.layers.0.self_attn.v.bias\", \"encoder.layers.0.self_attn.o.weight\", \"encoder.layers.0.self_attn.o.bias\", \"encoder.layers.0.positionwise_feedforward.fc_1.weight\", \"encoder.layers.0.positionwise_feedforward.fc_1.bias\", \"encoder.layers.0.positionwise_feedforward.fc_2.weight\", \"encoder.layers.0.positionwise_feedforward.fc_2.bias\", \"encoder.layers.1.self_att_layer_norm.weight\", \"encoder.layers.1.self_att_layer_norm.bias\", \"encoder.layers.1.ff_layer_norm.weight\", \"encoder.layers.1.ff_layer_norm.bias\", \"encoder.layers.1.self_attn.q.weight\", \"encoder.layers.1.self_attn.q.bias\", \"encoder.layers.1.self_attn.k.weight\", \"encoder.layers.1.self_attn.k.bias\", \"encoder.layers.1.self_attn.v.weight\", \"encoder.layers.1.self_attn.v.bias\", \"encoder.layers.1.self_attn.o.weight\", \"encoder.layers.1.self_attn.o.bias\", \"encoder.layers.1.positionwise_feedforward.fc_1.weight\", \"encoder.layers.1.positionwise_feedforward.fc_1.bias\", \"encoder.layers.1.positionwise_feedforward.fc_2.weight\", \"encoder.layers.1.positionwise_feedforward.fc_2.bias\", \"encoder.layers.2.self_att_layer_norm.weight\", \"encoder.layers.2.self_att_layer_norm.bias\", \"encoder.layers.2.ff_layer_norm.weight\", \"encoder.layers.2.ff_layer_norm.bias\", \"encoder.layers.2.self_attn.q.weight\", \"encoder.layers.2.self_attn.q.bias\", \"encoder.layers.2.self_attn.k.weight\", \"encoder.layers.2.self_attn.k.bias\", \"encoder.layers.2.self_attn.v.weight\", \"encoder.layers.2.self_attn.v.bias\", \"encoder.layers.2.self_attn.o.weight\", \"encoder.layers.2.self_attn.o.bias\", \"encoder.layers.2.positionwise_feedforward.fc_1.weight\", \"encoder.layers.2.positionwise_feedforward.fc_1.bias\", \"encoder.layers.2.positionwise_feedforward.fc_2.weight\", \"encoder.layers.2.positionwise_feedforward.fc_2.bias\", \"decoder.embedding.weight\", \"decoder.pos_embedding.weight\", \"decoder.layers.0.multi_att.q.weight\", \"decoder.layers.0.multi_att.q.bias\", \"decoder.layers.0.multi_att.k.weight\", \"decoder.layers.0.multi_att.k.bias\", \"decoder.layers.0.multi_att.v.weight\", \"decoder.layers.0.multi_att.v.bias\", \"decoder.layers.0.multi_att.o.weight\", \"decoder.layers.0.multi_att.o.bias\", \"decoder.layers.0.multi_att_norm.weight\", \"decoder.layers.0.multi_att_norm.bias\", \"decoder.layers.0.feed_forward_layer.fc_1.weight\", \"decoder.layers.0.feed_forward_layer.fc_1.bias\", \"decoder.layers.0.feed_forward_layer.fc_2.weight\", \"decoder.layers.0.feed_forward_layer.fc_2.bias\", \"decoder.layers.0.feed_norm.weight\", \"decoder.layers.0.feed_norm.bias\", \"decoder.layers.0.encoder_attn.q.weight\", \"decoder.layers.0.encoder_attn.q.bias\", \"decoder.layers.0.encoder_attn.k.weight\", \"decoder.layers.0.encoder_attn.k.bias\", \"decoder.layers.0.encoder_attn.v.weight\", \"decoder.layers.0.encoder_attn.v.bias\", \"decoder.layers.0.encoder_attn.o.weight\", \"decoder.layers.0.encoder_attn.o.bias\", \"decoder.layers.0.encoder_norm.weight\", \"decoder.layers.0.encoder_norm.bias\", \"decoder.layers.1.multi_att.q.weight\", \"decoder.layers.1.multi_att.q.bias\", \"decoder.layers.1.multi_att.k.weight\", \"decoder.layers.1.multi_att.k.bias\", \"decoder.layers.1.multi_att.v.weight\", \"decoder.layers.1.multi_att.v.bias\", \"decoder.layers.1.multi_att.o.weight\", \"decoder.layers.1.multi_att.o.bias\", \"decoder.layers.1.multi_att_norm.weight\", \"decoder.layers.1.multi_att_norm.bias\", \"decoder.layers.1.feed_forward_layer.fc_1.weight\", \"decoder.layers.1.feed_forward_layer.fc_1.bias\", \"decoder.layers.1.feed_forward_layer.fc_2.weight\", \"decoder.layers.1.feed_forward_layer.fc_2.bias\", \"decoder.layers.1.feed_norm.weight\", \"decoder.layers.1.feed_norm.bias\", \"decoder.layers.1.encoder_attn.q.weight\", \"decoder.layers.1.encoder_attn.q.bias\", \"decoder.layers.1.encoder_attn.k.weight\", \"decoder.layers.1.encoder_attn.k.bias\", \"decoder.layers.1.encoder_attn.v.weight\", \"decoder.layers.1.encoder_attn.v.bias\", \"decoder.layers.1.encoder_attn.o.weight\", \"decoder.layers.1.encoder_attn.o.bias\", \"decoder.layers.1.encoder_norm.weight\", \"decoder.layers.1.encoder_norm.bias\", \"decoder.layers.2.multi_att.q.weight\", \"decoder.layers.2.multi_att.q.bias\", \"decoder.layers.2.multi_att.k.weight\", \"decoder.layers.2.multi_att.k.bias\", \"decoder.layers.2.multi_att.v.weight\", \"decoder.layers.2.multi_att.v.bias\", \"decoder.layers.2.multi_att.o.weight\", \"decoder.layers.2.multi_att.o.bias\", \"decoder.layers.2.multi_att_norm.weight\", \"decoder.layers.2.multi_att_norm.bias\", \"decoder.layers.2.feed_forward_layer.fc_1.weight\", \"decoder.layers.2.feed_forward_layer.fc_1.bias\", \"decoder.layers.2.feed_forward_layer.fc_2.weight\", \"decoder.layers.2.feed_forward_layer.fc_2.bias\", \"decoder.layers.2.feed_norm.weight\", \"decoder.layers.2.feed_norm.bias\", \"decoder.layers.2.encoder_attn.q.weight\", \"decoder.layers.2.encoder_attn.q.bias\", \"decoder.layers.2.encoder_attn.k.weight\", \"decoder.layers.2.encoder_attn.k.bias\", \"decoder.layers.2.encoder_attn.v.weight\", \"decoder.layers.2.encoder_attn.v.bias\", \"decoder.layers.2.encoder_attn.o.weight\", \"decoder.layers.2.encoder_attn.o.bias\", \"decoder.layers.2.encoder_norm.weight\", \"decoder.layers.2.encoder_norm.bias\", \"decoder.fc_out.weight\", \"decoder.fc_out.bias\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut6-model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "src=next(iter(test_ds))['cs']\n",
    "trg=next(iter(test_ds))['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cs_tokenizer.encode(src).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src='Bílá kočka a černá kočka'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_pad_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "tids=torch.LongTensor(en_tokenizer.encode(trg).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tids!=trg_pad_id).unsqueeze(1).unsqueeze(2).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ True, False, False,  ..., False, False, False],\n",
       "          [ True,  True, False,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          ...,\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False],\n",
       "          [ True,  True,  True,  ..., False, False, False]]]], device='cuda:0')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_trg_mask_(tids.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trg_mask_(trg):\n",
    "        trg_pad_mask = (trg != trg_pad_id).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).bool().to(device)\n",
    "        # batch_size 1 seq_len seq_len\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, device, max_len = 50):\n",
    "    \n",
    "    model.eval()    \n",
    "        \n",
    "    src_indexes = cs_tokenizer.encode('<SOS> '+sentence+' <EOS>').ids\n",
    "\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "    # print(src_tensor)\n",
    "    \n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_sos_id]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        # print(trg_tensor.shape)\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "        # print(trg_mask)\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "        # print(output.shape)\n",
    "        pred_token = output.argmax(2)[:,-1].item()\n",
    "        # print(pred_token)\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_eos_id:\n",
    "            break\n",
    "    \n",
    "    trg_tokens = en_tokenizer.decode(trg_indexes)\n",
    "    \n",
    "    return trg_tokens,trg_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('He accused his friend Milan metro following emergency disaster .',\n",
       " [3, 1361, 7387, 951, 2214, 14669, 12105, 1704, 3967, 3876, 15, 1])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_sentence(src,model,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 911 Call, Professor Admits to Shooting Girlfriend'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = torch.randn(1,1,1,16940)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16940])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sth.argmax(2)[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "38d51ffecfa6c94f622a6e23c139cb10361cd4f5e925c79a2ea7c8dd5066085f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
